{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79e76aea5289e07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09b2701416e0096",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:37:25.103220Z",
     "start_time": "2025-05-27T16:36:43.185345Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "from src.helpers import text2list\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "arxiv_fn = \"../data/clean/arxiv_2018-01-01_2025-05-20_cs__.jsonl\"\n",
    "nyt_fn = \"../data/clean/nyt_2018-01-01_2025-05-20.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "ai_terms = \"../data/raw/raw_ai_terms.json\"\n",
    "with open(ai_terms, 'r') as f:\n",
    "    ai_terms = json.load(f)\n",
    "    \n",
    "\n",
    "anthro = ai_terms['anthro_score']\n",
    "liu = ai_terms['liu_et_al_subset']\n",
    "base = ['artificial intelligence', \"a.i.\"]\n",
    "\n",
    "all_terms = list(set(list(anthro) + list(liu))) + base\n",
    "\n",
    "# all_terms = list(ai_terms.values())\n",
    "# all_terms = [item for sublist in all_terms for item in sublist]\n",
    "# all_terms = [x.lower() for x in all_terms if isinstance(x, str)]\n",
    "# all_terms = list(set(all_terms))\n",
    "\n",
    "atus_roles = text2list(\"../data/clean/atus_roles.txt\")\n",
    "onet_roles = text2list(\"../data/clean/onet_roles.txt\")\n",
    "nouns = ['advice', 'feedback', 'support', 'guidance', 'encouragement', 'trust', 'communication', 'interaction', 'collaboration', 'relationship', 'connection', 'understanding', 'empathy', 'mentorship', 'network', 'rapport', 'bond', 'influence', 'cooperation', 'engagement']\n",
    "\n",
    "role_predicates = [\"[TERM]-powered [ROLE]\", \n",
    "                   \"[TERM]-generated [ROLE]\", \n",
    "                     \"[TERM]-driven [ROLE]\",\n",
    "                    \"[TERM]-augmented [ROLE]\",\n",
    "                        \"[TERM]-assisted [ROLE]\",\n",
    "                   \"[TERM] [ROLE\"]\n",
    "\n",
    "noun_predicates  = [\"[NOUN] from [TERM]\", \n",
    "                    \"[NOUN] with [TERM]\",\n",
    "                    \"[NOUN] using [TERM]\",\n",
    "                    \"[NOUN] via [TERM]\",\n",
    "                    \"[NOUN] through [TERM]\",\n",
    "                    \"[TERM] [NOUN]\"\n",
    "                   ]\n",
    "\n",
    "ai_compound_roles = [\n",
    "    predicate.replace(\"[TERM]\", term).replace(\"[ROLE]\", role)\n",
    "    for term in all_terms\n",
    "    for role in atus_roles + onet_roles + nouns\n",
    "    for predicate in role_predicates\n",
    "]\n",
    "\n",
    "ai_compound_nouns = [\n",
    "    predicate.replace(\"[TERM]\", term).replace(\"[NOUN]\", noun)\n",
    "    for term in all_terms\n",
    "    for noun in nouns\n",
    "    for predicate in noun_predicates\n",
    "]\n",
    "\n",
    "roles = atus_roles + onet_roles + nouns\n",
    "\n",
    "arxiv_df = pd.read_json(arxiv_fn, lines=True).sample(frac=0.5)\n",
    "arxiv_df['text'] = arxiv_df['title'] + \" \" + arxiv_df['abstract'] \n",
    "\n",
    "nyt_df = pd.read_json(nyt_fn, lines=True).sample(frac=0.5)\n",
    "nyt_df['text'] = nyt_df['headline'] + \" \" + nyt_df['abstract'] + \" \" + nyt_df['snippet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c64d11f3669d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c8e46173490f8f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:42:34.710376Z",
     "start_time": "2025-05-27T16:37:28.415993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastFlashTextCounter initialized with word lists.\n",
      "Counting keywords for 'ai'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09b98d0b7f0b4e9abb10b50d71a61b9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d237cda7b16426288dfa673775a41d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf0721e58f5b4008b441a600f6249001"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cd22acdc0154f8a87e4ad86ca9a3f2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'ai' completed.\n",
      "Counting keywords for 'social'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a4a88df687c4bd1bb45ef129612bec3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9c6ad67069941ccad8dc993b036d57d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cac1108169e4a639b924549c1d2c3e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d6f3e7365c74f02a8f0e5fef9bc6d1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'social' completed.\n",
      "Counting keywords for 'ai_compound_roles'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d07ad540c8b8405186de43da4119c091"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2984dd3b7b374b28bdc922d4d60b7ac3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4b8a6f0c5de46e78bc35c19e0ef4f07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea573c4f50834ee59b5049d4b6b7a1c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'ai_compound_roles' completed.\n",
      "Counting keywords for 'ai_compound_nouns'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b6cfdeee3d24dd69a77f7f285a1983e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "270afb6e6ec24fb1a6931aaa379f49e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fb78d7ac505482f8f9272f34ceae241"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffb2fd2e80e7448696871b4ed438829f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'ai_compound_nouns' completed.\n"
     ]
    }
   ],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import swifter \n",
    "\n",
    "class FastFlashTextCounter:\n",
    "\n",
    "    def __init__(self, word_lists_dict):\n",
    "        self.processors = {}\n",
    "\n",
    "        for name, word_list in word_lists_dict.items():\n",
    "            processor = KeywordProcessor(case_sensitive=False)\n",
    "            for word in word_list:\n",
    "                processor.add_keyword(word.lower())\n",
    "            self.processors[name] = processor\n",
    "        print(\"FastFlashTextCounter initialized with word lists.\")\n",
    "\n",
    "    def count_keywords(self, text, processor_name):\n",
    "        if pd.isna(text) or not text:\n",
    "            return {}\n",
    "\n",
    "        keywords_found = self.processors[processor_name].extract_keywords(str(text).lower())\n",
    "        return dict(Counter(keywords_found))\n",
    "\n",
    "\n",
    "word_lists = {\n",
    "    'ai': all_terms,\n",
    "    'social': roles, \n",
    "    'ai_compound_roles': ai_compound_roles,\n",
    "    'ai_compound_nouns': ai_compound_nouns\n",
    "}\n",
    "counter = FastFlashTextCounter(word_lists)\n",
    "\n",
    "\n",
    "\n",
    "for name, word_list in word_lists.items():\n",
    "    print(f\"Counting keywords for '{name}'...\")\n",
    "    arxiv_df[f'{name}_word_counts'] = arxiv_df['text'].swifter.apply(lambda x: counter.count_keywords(x, name))\n",
    "    arxiv_df[f'{name}_sum'] = arxiv_df[f'{name}_word_counts'].swifter.apply(lambda x: sum(x.values()))\n",
    "    \n",
    "    nyt_df[f'{name}_word_counts'] = nyt_df['text'].swifter.apply(lambda x: counter.count_keywords(x, name))\n",
    "    nyt_df[f'{name}_sum'] = nyt_df[f'{name}_word_counts'].swifter.apply(lambda x: sum(x.values()))\n",
    "    print(f\"Keyword counts for '{name}' completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525ea268d684c896",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:43:19.456980Z",
     "start_time": "2025-05-27T16:43:19.322831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  id     unique_idx         ai_sum     social_sum  \\\ncount  302359.000000  302359.000000  302359.000000  302359.000000   \nmean     2198.603932  302626.405389       0.384503       0.909224   \nstd       206.465534  174552.688353       1.311009       1.543530   \nmin      1801.003870       3.000000       0.000000       0.000000   \n25%      2010.058750  151408.000000       0.000000       0.000000   \n50%      2209.086480  302793.000000       0.000000       0.000000   \n75%      2403.141120  453928.000000       0.000000       1.000000   \nmax      2505.105660  604717.000000      21.000000      25.000000   \n\n       ai_compound_roles_sum  ai_compound_nouns_sum  \ncount          302359.000000          302359.000000  \nmean                0.000003               0.000628  \nstd                 0.001819               0.026844  \nmin                 0.000000               0.000000  \n25%                 0.000000               0.000000  \n50%                 0.000000               0.000000  \n75%                 0.000000               0.000000  \nmax                 1.000000               3.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>unique_idx</th>\n      <th>ai_sum</th>\n      <th>social_sum</th>\n      <th>ai_compound_roles_sum</th>\n      <th>ai_compound_nouns_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>302359.000000</td>\n      <td>302359.000000</td>\n      <td>302359.000000</td>\n      <td>302359.000000</td>\n      <td>302359.000000</td>\n      <td>302359.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2198.603932</td>\n      <td>302626.405389</td>\n      <td>0.384503</td>\n      <td>0.909224</td>\n      <td>0.000003</td>\n      <td>0.000628</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>206.465534</td>\n      <td>174552.688353</td>\n      <td>1.311009</td>\n      <td>1.543530</td>\n      <td>0.001819</td>\n      <td>0.026844</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1801.003870</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2010.058750</td>\n      <td>151408.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2209.086480</td>\n      <td>302793.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2403.141120</td>\n      <td>453928.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2505.105660</td>\n      <td>604717.000000</td>\n      <td>21.000000</td>\n      <td>25.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d7d4f0628b780d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:27:49.716082Z",
     "start_time": "2025-05-27T16:27:49.694760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 2, 'granny smith': 1}\n"
     ]
    }
   ],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FastFlashTextCounter:\n",
    "\n",
    "    def __init__(self, word_lists_dict):\n",
    "        self.word_processors = {}\n",
    "        self.bigram_processors = {}\n",
    "\n",
    "        for name, word_list in word_lists_dict.items():\n",
    "            # Processor for single words\n",
    "            word_processor = KeywordProcessor(case_sensitive=False)\n",
    "            for word in word_list:\n",
    "                word_processor.add_keyword(word.lower())\n",
    "            self.word_processors[name] = word_processor\n",
    "\n",
    "            # Processor for bigrams\n",
    "            bigram_processor = KeywordProcessor(case_sensitive=False)\n",
    "            bigrams = [f\"{word_list[i]} {word_list[j]}\" for i in range(len(word_list)) for j in range(i + 1, len(word_list))]\n",
    "            for bigram in bigrams:\n",
    "                bigram_processor.add_keyword(bigram.lower())\n",
    "            self.bigram_processors[name] = bigram_processor\n",
    "\n",
    "    def count_keywords(self, text, processor_name):\n",
    "        if pd.isna(text) or not text:\n",
    "            return {}\n",
    "\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        word_counts = Counter(self.word_processors[processor_name].extract_keywords(text_lower))\n",
    "\n",
    "        bigram_counts = Counter(self.bigram_processors[processor_name].extract_keywords(text_lower))\n",
    "\n",
    "        total_counts = word_counts + bigram_counts\n",
    "        return dict(total_counts)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "word_lists = {\n",
    "    'apple': ['apple', 'granny smith'],\n",
    "}\n",
    "counter = FastFlashTextCounter(word_lists)\n",
    "\n",
    "sample_text = [\"I had an apple that was tasty. It was a granny smith apple.\"]\n",
    "result = counter.count_keywords(sample_text[0], 'apple')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a464554317c2d48",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Method Implementations ---\n",
    "\n",
    "# M0: Pure Python - Naive str.count (Substring, Flawed)\n",
    "def count_words_M0(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text) # Ensure string\n",
    "        current_counts = base_dict.copy()\n",
    "        for term in word_list_to_count:\n",
    "            current_counts[term] = text_str.count(term) # Substring count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M1: Pure Python - Row-wise re.findall (One Regex Per Term)\n",
    "def count_words_M1(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    # Pre-compile regexes for each term\n",
    "    term_regexes = {term: re.compile(r'\\b' + re.escape(term) + r'\\b') for term in word_list_to_count}\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        for term, term_re in term_regexes.items():\n",
    "            current_counts[term] = len(term_re.findall(text_str))\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M2: Pure Python - Row-wise Single Compiled Regex\n",
    "COMPILED_REGEX_M2 = re.compile(r'\\b(' + '|'.join(re.escape(term) for term in WORD_LIST) + r')\\b')\n",
    "def count_words_M2(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    regex_pattern = COMPILED_REGEX_M2 # Uses global WORD_LIST based pattern\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        found_words = regex_pattern.findall(text_str)\n",
    "        if found_words:\n",
    "            counts_in_row = Counter(found_words)\n",
    "            for word, count in counts_in_row.items():\n",
    "                if word in current_counts:\n",
    "                    current_counts[word] = count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M3: Pandas apply - Single Compiled Regex\n",
    "def count_words_M3(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    regex_pattern = COMPILED_REGEX_M2 # Uses global WORD_LIST based pattern\n",
    "    \n",
    "    def process_row(text):\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        found_words = regex_pattern.findall(text_str)\n",
    "        if found_words:\n",
    "            counts_in_row = Counter(found_words)\n",
    "            for word, count in counts_in_row.items():\n",
    "                if word in current_counts:\n",
    "                    current_counts[word] = count\n",
    "        return current_counts\n",
    "        \n",
    "    results = df_input['text'].apply(process_row).tolist()\n",
    "    return results\n",
    "\n",
    "# M4: Pandas Vectorized - str.findall with Single Compiled Regex (Original Best)\n",
    "def count_words_M4(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    regex_pattern_str = r'\\b(' + '|'.join(re.escape(term) for term in word_list_to_count) + r')\\b' # word_list_to_count is already sorted by length desc\n",
    "    \n",
    "    all_found_words_series = df_input['text'].astype(str).str.findall(regex_pattern_str)\n",
    "    \n",
    "    results_list = []\n",
    "    for list_of_matches_in_row in all_found_words_series:\n",
    "        current_row_counts = base_dict.copy()\n",
    "        if list_of_matches_in_row:\n",
    "            term_counts_in_row = Counter(list_of_matches_in_row)\n",
    "            for term, count in term_counts_in_row.items():\n",
    "                if term in current_row_counts:\n",
    "                    current_row_counts[term] = count\n",
    "        results_list.append(current_row_counts)\n",
    "    return results_list\n",
    "\n",
    "# M5: Pandas Vectorized - str.count (One Regex Per Term, then combine)\n",
    "def count_words_M5(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    # Create a DataFrame to hold counts for each term\n",
    "    counts_df = pd.DataFrame(index=df_input.index)\n",
    "    for term in word_list_to_count:\n",
    "        term_pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        counts_df[term] = df_input['text'].astype(str).str.count(term_pattern)\n",
    "    \n",
    "    # Convert the counts_df to list of dicts\n",
    "    results = counts_df.to_dict(orient='records')\n",
    "    return results\n",
    "\n",
    "# M6: flashtext Library\n",
    "try:\n",
    "    from flashtext import KeywordProcessor\n",
    "    FLASHTEXT_LOADED = True\n",
    "except ImportError:\n",
    "    FLASHTEXT_LOADED = False\n",
    "    print(\"FlashText not installed. Skipping M6.\")\n",
    "\n",
    "def count_words_M6(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    if not FLASHTEXT_LOADED:\n",
    "        return [BASE_COUNTS_DICT.copy() for _ in range(len(df_input))] # Return dummy if not loaded\n",
    "\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=True)\n",
    "    for term in word_list_to_count:\n",
    "        keyword_processor.add_keyword(term, term) # Map term to itself\n",
    "\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        found_terms = keyword_processor.extract_keywords(text_str) # Returns list of mapped values (terms themselves)\n",
    "        if found_terms:\n",
    "            counts_in_row = Counter(found_terms)\n",
    "            for term, count in counts_in_row.items():\n",
    "                if term in current_counts:\n",
    "                    current_counts[term] = count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M7: CountVectorizer (Custom token_pattern)\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    SKLEARN_LOADED = True\n",
    "except ImportError:\n",
    "    SKLEARN_LOADED = False\n",
    "    print(\"Scikit-learn not installed. Skipping M7.\")\n",
    "\n",
    "def count_words_M7(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    if not SKLEARN_LOADED:\n",
    "         return [BASE_COUNTS_DICT.copy() for _ in range(len(df_input))]\n",
    "\n",
    "    # word_list_to_count is already sorted by length desc globally (WORD_LIST)\n",
    "    regex_for_cv_tokens = r'\\b(?:' + '|'.join(re.escape(term) for term in word_list_to_count) + r')\\b'\n",
    "    \n",
    "    cv = CountVectorizer(token_pattern=regex_for_cv_tokens, lowercase=False)\n",
    "    \n",
    "    # Fit and transform\n",
    "    X = cv.fit_transform(df_input['text'].astype(str))\n",
    "    # Get the vocabulary that CountVectorizer actually built (these are our terms if found)\n",
    "    fitted_cv_vocab = cv.get_feature_names_out()\n",
    "    \n",
    "    results = []\n",
    "    # Ensure all terms from original word_list_to_count are in each dict\n",
    "    for i in range(X.shape[0]):\n",
    "        row_counts = {term: 0 for term in word_list_to_count}\n",
    "        doc_vector = X[i]\n",
    "        if doc_vector.nnz > 0: # If any non-zero elements (terms found)\n",
    "            for term_idx, count in zip(doc_vector.indices, doc_vector.data):\n",
    "                term = fitted_cv_vocab[term_idx]\n",
    "                if term in row_counts: # Should always be true if word_list_to_count was basis\n",
    "                    row_counts[term] = count\n",
    "        results.append(row_counts)\n",
    "    return results\n",
    "\n",
    "\n",
    "# M8: Pure Python - Tokenize then Match N-grams\n",
    "def simple_tokenizer(text: str) -> list[str]:\n",
    "    # Basic tokenizer, splits by space and removes empty strings\n",
    "    return [token for token in text.split(' ') if token]\n",
    "\n",
    "def count_words_M8(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    \n",
    "    # Pre-split terms in word_list\n",
    "    split_word_list = {term: term.split(' ') for term in word_list_to_count}\n",
    "\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        # For this method, a simple split might be better to match n-grams constructed by space\n",
    "        text_tokens = simple_tokenizer(text_str) \n",
    "        \n",
    "        if not text_tokens: # Handle empty text\n",
    "            results.append(current_counts)\n",
    "            continue\n",
    "\n",
    "        for term, term_tokens in split_word_list.items():\n",
    "            n_term_tokens = len(term_tokens)\n",
    "            if n_term_tokens == 0: continue\n",
    "\n",
    "            count = 0\n",
    "            for i in range(len(text_tokens) - n_term_tokens + 1):\n",
    "                if text_tokens[i:i+n_term_tokens] == term_tokens:\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                current_counts[term] = count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# --- Multiprocessing Helper ---\n",
    "def process_chunk_M2(text_chunk_list: list[str]) -> list[dict[str, int]]:\n",
    "    # This function will be mapped; it needs to be self-contained or use globals carefully\n",
    "    # WORD_LIST and COMPILED_REGEX_M2 are global in the main process\n",
    "    # For multiprocessing, it's better to pass such things or re-initialize if small\n",
    "    # Here, COMPILED_REGEX_M2 is defined from global WORD_LIST.\n",
    "    # Re-define for safety in new processes if needed, or ensure it's inherited.\n",
    "    # Python's multiprocessing on Unix often uses fork, so globals might be available.\n",
    "    # On Windows, it pickles, so globals need to be picklable or passed.\n",
    "    # Let's assume it works or pass WORD_LIST if issues.\n",
    "\n",
    "    current_regex = re.compile(r'\\b(' + '|'.join(re.escape(term) for term in WORD_LIST) + r')\\b')\n",
    "    base_d = {word: 0 for word in WORD_LIST}\n",
    "    chunk_results = []\n",
    "    for text in text_chunk_list:\n",
    "        text_str = str(text)\n",
    "        current_row_counts = base_d.copy()\n",
    "        found_words = current_regex.findall(text_str)\n",
    "        if found_words:\n",
    "            term_counts_in_row = Counter(found_words)\n",
    "            for term, count_val in term_counts_in_row.items():\n",
    "                if term in current_row_counts:\n",
    "                    current_row_counts[term] = count_val\n",
    "        chunk_results.append(current_row_counts)\n",
    "    return chunk_results\n",
    "\n",
    "def count_words_M9_worker(df_chunk): # M2 logic on a df chunk\n",
    "    return count_words_M2(df_chunk, WORD_LIST)\n",
    "\n",
    "\n",
    "def count_words_M9(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    # M2 logic (Python loop + single regex) parallelized\n",
    "    text_list = df_input['text'].tolist()\n",
    "    chunk_size = max(1, len(text_list) // NUM_PROCESSES)\n",
    "    chunks = [text_list[i:i + chunk_size] for i in range(0, len(text_list), chunk_size)]\n",
    "    \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        list_of_results_chunks = pool.map(process_chunk_M2, chunks)\n",
    "    \n",
    "    final_results = [item for sublist in list_of_results_chunks for item in sublist]\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def count_words_M10_worker(df_chunk): # M4 logic on a df chunk\n",
    "    return count_words_M4(df_chunk, WORD_LIST)\n",
    "\n",
    "def count_words_M10(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    # M4 logic (Pandas str.findall) parallelized\n",
    "    # Splitting a DataFrame for multiprocessing\n",
    "    df_chunks = np.array_split(df_input, NUM_PROCESSES)\n",
    "    \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        list_of_results_chunks = pool.map(count_words_M10_worker, df_chunks)\n",
    "        \n",
    "    final_results = [item for sublist in list_of_results_chunks for item in sublist]\n",
    "    return final_results\n",
    "\n",
    "# --- Timing and Execution ---\n",
    "methods_to_time = {\n",
    "    \"M0_Loop_StrCount\": count_words_M0,\n",
    "    \"M1_Loop_ReFindall_PerTerm\": count_words_M1,\n",
    "    \"M2_Loop_SingleReFindall\": count_words_M2,\n",
    "    \"M3_PandasApply_SingleRe\": count_words_M3,\n",
    "    \"M4_Pandas_strFindall_SingleRe\": count_words_M4,\n",
    "    \"M5_Pandas_strCount_PerTerm\": count_words_M5,\n",
    "    \"M8_PyLoop_TokenizeMatch\": count_words_M8, # Potentially very slow\n",
    "}\n",
    "if FLASHTEXT_LOADED:\n",
    "    methods_to_time[\"M6_FlashText\"] = count_words_M6\n",
    "if SKLEARN_LOADED:\n",
    "    methods_to_time[\"M7_CountVectorizer\"] = count_words_M7\n",
    "\n",
    "# Multiprocessing methods added separately due to potential for long setup/run times\n",
    "# or if user wants to skip them.\n",
    "# For a fair comparison, the non-parallelized versions are more direct unless specifically testing parallel overhead.\n",
    "# Adding them if explicitly requested or as part of a comprehensive test.\n",
    "# methods_to_time[\"M9_MP_M2\"] = count_words_M9\n",
    "# methods_to_time[\"M10_MP_M4\"] = count_words_M10\n",
    "\n",
    "\n",
    "timings = {}\n",
    "# To ensure correctness, let's get a reference result from one reliable method (M4) on a small subset\n",
    "# And verify other methods against it (structure check done globally)\n",
    "# Check results for first few rows from one method to ensure format\n",
    "# results_m4_sample = count_words_M4(df_main.head(), WORD_LIST)\n",
    "# check_output(results_m4_sample, WORD_LIST)\n",
    "\n",
    "\n",
    "print(f\"\\n--- Starting Benchmark on {NUM_ROWS} rows ---\")\n",
    "print(f\"Using WORD_LIST: {WORD_LIST}\\n\")\n",
    "\n",
    "# Limit number of methods for practical timing in one go, especially slow ones\n",
    "# You can uncomment methods as needed. M0, M1, M8 can be extremely slow.\n",
    "# I'll run a subset that are more likely to be practical.\n",
    "methods_to_run = {\n",
    "    # \"M0_Loop_StrCount\": count_words_M0, # Likely very slow and flawed\n",
    "    \"M1_Loop_ReFindall_PerTerm\": count_words_M1, # Likely very slow\n",
    "    # \"M2_Loop_SingleReFindall\": count_words_M2,\n",
    "    # \"M3_PandasApply_SingleRe\": count_words_M3,\n",
    "    # \"M4_Pandas_strFindall_SingleRe\": count_words_M4, # Expected best\n",
    "    # \"M5_Pandas_strCount_PerTerm\": count_words_M5,\n",
    "}\n",
    "if FLASHTEXT_LOADED:\n",
    "    pass\n",
    "    # methods_to_run[\"M6_FlashText\"] = count_words_M6\n",
    "if SKLEARN_LOADED:\n",
    "    pass\n",
    "    # methods_to_run[\"M7_CountVectorizer\"] = count_words_M7\n",
    "# \"M8_PyLoop_TokenizeMatch\": count_words_M8, # Likely very slow\n",
    "# Add multiprocessing if you want to test their overhead and scaling\n",
    "methods_to_run[\"M9_MP_M2\"] = count_words_M9\n",
    "methods_to_run[\"M10_MP_M4\"] = count_words_M10\n",
    "\n",
    "\n",
    "for name, method_func in methods_to_run.items():\n",
    "    print(f\"Timing {name}...\")\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        # Execute the method\n",
    "        results = method_func(df_main, WORD_LIST)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "        timings[name] = elapsed_time\n",
    "        print(f\"{name} took: {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        # Basic validation of output structure (can be commented out for speed after first check)\n",
    "        if not check_output(results, WORD_LIST):\n",
    "             print(f\"!! Output validation failed for {name}\")\n",
    "        # Optional: check if len of results matches df_main\n",
    "        if len(results) != len(df_main):\n",
    "            print(f\"!! Length mismatch for {name}: expected {len(df_main)}, got {len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {name}: {e}\")\n",
    "        timings[name] = \"Error\"\n",
    "\n",
    "print(\"\\n--- Benchmark Results ---\")\n",
    "for name, t in timings.items():\n",
    "    if isinstance(t, str): # Error case\n",
    "        print(f\"{name}: {t}\")\n",
    "    else:\n",
    "        print(f\"{name}: {t:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04ad0973f4eacd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "def create_fast_counter(word_list):\n",
    "    \"\"\"\n",
    "    Create a fast word counter function using precompiled regex.\n",
    "    \n",
    "    Args:\n",
    "        word_list (list): List of words to count\n",
    "    \n",
    "    Returns:\n",
    "        function: Optimized counting function\n",
    "    \"\"\"\n",
    "    # Preprocess words\n",
    "    words_lower = [word.lower() for word in word_list]\n",
    "    \n",
    "    # Create all n-grams (1-grams and 2-grams)\n",
    "    ngrams = words_lower + [' '.join(pair) for pair in combinations(words_lower, 2)]\n",
    "    \n",
    "    # Sort by length (longest first) to avoid partial matches\n",
    "    ngrams.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Precompile regex pattern\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(ngram) for ngram in ngrams) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "    def count_matches(text):\n",
    "        if pd.isna(text) or not text:\n",
    "            return {}\n",
    "        return dict(Counter(match.lower() for match in pattern.findall(str(text))))\n",
    "    \n",
    "    return count_matches\n",
    "\n",
    "# Create optimized counters\n",
    "fast_ai_counter = create_fast_counter(all_terms)\n",
    "fast_social_counter = create_fast_counter(roles)\n",
    "\n",
    "# Apply to dataframe (much faster than swifter for this use case)\n",
    "arxiv_df['ai_word_counts'] = arxiv_df['text'].apply(fast_ai_counter)\n",
    "arxiv_df['social_word_counts'] = arxiv_df['text'].apply(fast_social_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883265b5de41032a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

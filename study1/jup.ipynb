{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79e76aea5289e07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09b2701416e0096",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T17:11:29.271919Z",
     "start_time": "2025-05-27T17:10:31.998768Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "from src.helpers import text2list\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "arxiv_fn = \"../data/clean/arxiv_2018-01-01_2025-05-20_cs__.jsonl\"\n",
    "nyt_fn = \"../data/clean/nyt_2018-01-01_2025-05-20.jsonl\"\n",
    "\n",
    "\n",
    "ai_terms = text2list(\"../data/clean/ai_terms.txt\")\n",
    "\n",
    "# all_terms = list(ai_terms.values())\n",
    "# all_terms = [item for sublist in all_terms for item in sublist]\n",
    "# all_terms = [x.lower() for x in all_terms if isinstance(x, str)]\n",
    "# all_terms = list(set(all_terms))\n",
    "\n",
    "atus_roles = text2list(\"../data/clean/atus_roles.txt\")\n",
    "onet_roles = text2list(\"../data/clean/onet_roles.txt\")\n",
    "nouns = ['advice', 'feedback', 'support', 'guidance', 'encouragement', 'trust', 'communication', 'interaction', 'collaboration', 'relationship', 'connection', 'understanding', 'empathy', 'mentorship', 'network', 'rapport', 'bond', 'influence', 'cooperation', 'engagement']\n",
    "\n",
    "ai_compound_roles = text2list(\"../data/clean/ai_compound_roles.txt\")\n",
    "ai_compound_nouns = text2list(\"../data/clean/ai_compound_nouns.txt\")\n",
    "\n",
    "ai_compound = ai_compound_roles + ai_compound_nouns\n",
    "\n",
    "arxiv_df = pd.read_json(arxiv_fn, lines=True).sample(frac=0.5)\n",
    "arxiv_df['text'] = arxiv_df['title'] + \" \" + arxiv_df['abstract'] \n",
    "\n",
    "nyt_df = pd.read_json(nyt_fn, lines=True).sample(frac=0.5)\n",
    "nyt_df['text'] = nyt_df['headline'] + \" \" + nyt_df['abstract'] + \" \" + nyt_df['snippet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c64d11f3669d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c8e46173490f8f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T17:20:19.216021Z",
     "start_time": "2025-05-27T17:11:33.270524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastFlashTextCounter initialized with word lists.\n",
      "Counting keywords for 'ai'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(32482) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7506e12044594982bf2fd3c762136cd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5016f5406b2a48b4a7dd01e04e0a2d46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fb9cfdee32243a1aa84287812398318"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f7ba77cc4e944f396aa64575b0b2242"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'ai' completed.\n",
      "Counting keywords for 'social'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ca994be8485401a94ef85792b68d9ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c35b7d8e15d4a4f8e35b56bcd417372"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d644d7d44bd4d29b661c0943666538e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ac157a606bc45c9a7eab92df6b6bd07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'social' completed.\n",
      "Counting keywords for 'ai_compound_roles'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fcf85d6ec4e45a3adbc0e2bb17b42f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ed84fc59ebf485d9f089204639c41c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e3544811c6e48f9a03387daa4dacbcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85c791e4f3c64d8e9c3c73e5778ceea6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'ai_compound_roles' completed.\n",
      "Counting keywords for 'ai_compound_nouns'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b79c56f5d2fc4916b3b3c135730e70ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/302359 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7651dbe04d8846bbb743975014409376"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74dd668e49694d08917a595bf7c6c1ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/149074 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2df45e93f61646f6a2c26bc0e41f5da3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword counts for 'ai_compound_nouns' completed.\n"
     ]
    }
   ],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import swifter \n",
    "\n",
    "class FastFlashTextCounter:\n",
    "\n",
    "    def __init__(self, word_lists_dict):\n",
    "        self.processors = {}\n",
    "\n",
    "        for name, word_list in word_lists_dict.items():\n",
    "            processor = KeywordProcessor(case_sensitive=False)\n",
    "            for word in word_list:\n",
    "                processor.add_keyword(word.lower())\n",
    "            self.processors[name] = processor\n",
    "        print(\"FastFlashTextCounter initialized with word lists.\")\n",
    "\n",
    "    def count_keywords(self, text, processor_name):\n",
    "        if pd.isna(text) or not text:\n",
    "            return {}\n",
    "\n",
    "        keywords_found = self.processors[processor_name].extract_keywords(str(text).lower())\n",
    "        return dict(Counter(keywords_found))\n",
    "\n",
    "\n",
    "word_lists = {\n",
    "    'ai': ai_terms,\n",
    "    'social':  atus_roles + onet_roles + nouns, \n",
    "    'ai_compound_roles': ai_compound_roles,\n",
    "    'ai_compound_nouns': ai_compound_nouns\n",
    "}\n",
    "counter = FastFlashTextCounter(word_lists)\n",
    "\n",
    "\n",
    "\n",
    "for name, word_list in word_lists.items():\n",
    "    print(f\"Counting keywords for '{name}'...\")\n",
    "    arxiv_df[f'{name}_word_counts'] = arxiv_df['text'].swifter.apply(lambda x: counter.count_keywords(x, name))\n",
    "    arxiv_df[f'{name}_sum'] = arxiv_df[f'{name}_word_counts'].swifter.apply(lambda x: sum(x.values()))\n",
    "    \n",
    "    nyt_df[f'{name}_word_counts'] = nyt_df['text'].swifter.apply(lambda x: counter.count_keywords(x, name))\n",
    "    nyt_df[f'{name}_sum'] = nyt_df[f'{name}_word_counts'].swifter.apply(lambda x: sum(x.values()))\n",
    "    print(f\"Keyword counts for '{name}' completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "525ea268d684c896",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T17:26:10.738643Z",
     "start_time": "2025-05-27T17:26:09.712484Z"
    }
   },
   "outputs": [],
   "source": [
    "def sum_dicts(list_of_dicts):\n",
    "    \"\"\"\n",
    "    Sums a list of dictionaries with the same keys.\n",
    "    \n",
    "    Args:\n",
    "        list_of_dicts (list): List of dictionaries to sum.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with summed values.\n",
    "    \"\"\"\n",
    "    if not list_of_dicts:\n",
    "        return {}\n",
    "    \n",
    "    total_counts = Counter()\n",
    "    for d in list_of_dicts:\n",
    "        total_counts.update(d)\n",
    "    \n",
    "    return sort_dict(dict(total_counts))\n",
    "\n",
    "def sort_dict(d):\n",
    "    \"\"\"\n",
    "    Sorts a dictionary by its values.\n",
    "    \n",
    "    Args:\n",
    "        d (dict): Dictionary to sort.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Sorted dictionary.\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "arxiv_ai_compound_nouns = sum_dicts(arxiv_df['ai_compound_nouns_word_counts'].tolist())\n",
    "nyt_ai_compound_nouns = sum_dicts(nyt_df['ai_compound_nouns_word_counts'].tolist())\n",
    "total_ai_compound_nouns = sum_dicts([arxiv_ai_compound_nouns, nyt_ai_compound_nouns])\n",
    "\n",
    "arxiv_ai_compound_roles = sum_dicts(arxiv_df['ai_compound_roles_word_counts'].tolist())\n",
    "nyt_ai_compound_roles = sum_dicts(nyt_df['ai_compound_roles_word_counts'].tolist())\n",
    "total_ai_compound_roles = sum_dicts([arxiv_ai_compound_roles, nyt_ai_compound_roles])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_dict(nyt_ai_compound_roles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T17:27:18.135725Z",
     "start_time": "2025-05-27T17:27:18.125107Z"
    }
   },
   "id": "1e605f4dc99575f",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3ebb4d4be4ee206"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d7d4f0628b780d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:27:49.716082Z",
     "start_time": "2025-05-27T16:27:49.694760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 2, 'granny smith': 1}\n"
     ]
    }
   ],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FastFlashTextCounter:\n",
    "\n",
    "    def __init__(self, word_lists_dict):\n",
    "        self.word_processors = {}\n",
    "        self.bigram_processors = {}\n",
    "\n",
    "        for name, word_list in word_lists_dict.items():\n",
    "            # Processor for single words\n",
    "            word_processor = KeywordProcessor(case_sensitive=False)\n",
    "            for word in word_list:\n",
    "                word_processor.add_keyword(word.lower())\n",
    "            self.word_processors[name] = word_processor\n",
    "\n",
    "            # Processor for bigrams\n",
    "            bigram_processor = KeywordProcessor(case_sensitive=False)\n",
    "            bigrams = [f\"{word_list[i]} {word_list[j]}\" for i in range(len(word_list)) for j in range(i + 1, len(word_list))]\n",
    "            for bigram in bigrams:\n",
    "                bigram_processor.add_keyword(bigram.lower())\n",
    "            self.bigram_processors[name] = bigram_processor\n",
    "\n",
    "    def count_keywords(self, text, processor_name):\n",
    "        if pd.isna(text) or not text:\n",
    "            return {}\n",
    "\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        word_counts = Counter(self.word_processors[processor_name].extract_keywords(text_lower))\n",
    "\n",
    "        bigram_counts = Counter(self.bigram_processors[processor_name].extract_keywords(text_lower))\n",
    "\n",
    "        total_counts = word_counts + bigram_counts\n",
    "        return dict(total_counts)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "word_lists = {\n",
    "    'apple': ['apple', 'granny smith'],\n",
    "}\n",
    "counter = FastFlashTextCounter(word_lists)\n",
    "\n",
    "sample_text = [\"I had an apple that was tasty. It was a granny smith apple.\"]\n",
    "result = counter.count_keywords(sample_text[0], 'apple')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a464554317c2d48",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Method Implementations ---\n",
    "\n",
    "# M0: Pure Python - Naive str.count (Substring, Flawed)\n",
    "def count_words_M0(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text) # Ensure string\n",
    "        current_counts = base_dict.copy()\n",
    "        for term in word_list_to_count:\n",
    "            current_counts[term] = text_str.count(term) # Substring count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M1: Pure Python - Row-wise re.findall (One Regex Per Term)\n",
    "def count_words_M1(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    # Pre-compile regexes for each term\n",
    "    term_regexes = {term: re.compile(r'\\b' + re.escape(term) + r'\\b') for term in word_list_to_count}\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        for term, term_re in term_regexes.items():\n",
    "            current_counts[term] = len(term_re.findall(text_str))\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M2: Pure Python - Row-wise Single Compiled Regex\n",
    "COMPILED_REGEX_M2 = re.compile(r'\\b(' + '|'.join(re.escape(term) for term in WORD_LIST) + r')\\b')\n",
    "def count_words_M2(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    regex_pattern = COMPILED_REGEX_M2 # Uses global WORD_LIST based pattern\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        found_words = regex_pattern.findall(text_str)\n",
    "        if found_words:\n",
    "            counts_in_row = Counter(found_words)\n",
    "            for word, count in counts_in_row.items():\n",
    "                if word in current_counts:\n",
    "                    current_counts[word] = count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M3: Pandas apply - Single Compiled Regex\n",
    "def count_words_M3(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    regex_pattern = COMPILED_REGEX_M2 # Uses global WORD_LIST based pattern\n",
    "    \n",
    "    def process_row(text):\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        found_words = regex_pattern.findall(text_str)\n",
    "        if found_words:\n",
    "            counts_in_row = Counter(found_words)\n",
    "            for word, count in counts_in_row.items():\n",
    "                if word in current_counts:\n",
    "                    current_counts[word] = count\n",
    "        return current_counts\n",
    "        \n",
    "    results = df_input['text'].apply(process_row).tolist()\n",
    "    return results\n",
    "\n",
    "# M4: Pandas Vectorized - str.findall with Single Compiled Regex (Original Best)\n",
    "def count_words_M4(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    regex_pattern_str = r'\\b(' + '|'.join(re.escape(term) for term in word_list_to_count) + r')\\b' # word_list_to_count is already sorted by length desc\n",
    "    \n",
    "    all_found_words_series = df_input['text'].astype(str).str.findall(regex_pattern_str)\n",
    "    \n",
    "    results_list = []\n",
    "    for list_of_matches_in_row in all_found_words_series:\n",
    "        current_row_counts = base_dict.copy()\n",
    "        if list_of_matches_in_row:\n",
    "            term_counts_in_row = Counter(list_of_matches_in_row)\n",
    "            for term, count in term_counts_in_row.items():\n",
    "                if term in current_row_counts:\n",
    "                    current_row_counts[term] = count\n",
    "        results_list.append(current_row_counts)\n",
    "    return results_list\n",
    "\n",
    "# M5: Pandas Vectorized - str.count (One Regex Per Term, then combine)\n",
    "def count_words_M5(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    # Create a DataFrame to hold counts for each term\n",
    "    counts_df = pd.DataFrame(index=df_input.index)\n",
    "    for term in word_list_to_count:\n",
    "        term_pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        counts_df[term] = df_input['text'].astype(str).str.count(term_pattern)\n",
    "    \n",
    "    # Convert the counts_df to list of dicts\n",
    "    results = counts_df.to_dict(orient='records')\n",
    "    return results\n",
    "\n",
    "# M6: flashtext Library\n",
    "try:\n",
    "    from flashtext import KeywordProcessor\n",
    "    FLASHTEXT_LOADED = True\n",
    "except ImportError:\n",
    "    FLASHTEXT_LOADED = False\n",
    "    print(\"FlashText not installed. Skipping M6.\")\n",
    "\n",
    "def count_words_M6(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    if not FLASHTEXT_LOADED:\n",
    "        return [BASE_COUNTS_DICT.copy() for _ in range(len(df_input))] # Return dummy if not loaded\n",
    "\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=True)\n",
    "    for term in word_list_to_count:\n",
    "        keyword_processor.add_keyword(term, term) # Map term to itself\n",
    "\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        found_terms = keyword_processor.extract_keywords(text_str) # Returns list of mapped values (terms themselves)\n",
    "        if found_terms:\n",
    "            counts_in_row = Counter(found_terms)\n",
    "            for term, count in counts_in_row.items():\n",
    "                if term in current_counts:\n",
    "                    current_counts[term] = count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# M7: CountVectorizer (Custom token_pattern)\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    SKLEARN_LOADED = True\n",
    "except ImportError:\n",
    "    SKLEARN_LOADED = False\n",
    "    print(\"Scikit-learn not installed. Skipping M7.\")\n",
    "\n",
    "def count_words_M7(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    if not SKLEARN_LOADED:\n",
    "         return [BASE_COUNTS_DICT.copy() for _ in range(len(df_input))]\n",
    "\n",
    "    # word_list_to_count is already sorted by length desc globally (WORD_LIST)\n",
    "    regex_for_cv_tokens = r'\\b(?:' + '|'.join(re.escape(term) for term in word_list_to_count) + r')\\b'\n",
    "    \n",
    "    cv = CountVectorizer(token_pattern=regex_for_cv_tokens, lowercase=False)\n",
    "    \n",
    "    # Fit and transform\n",
    "    X = cv.fit_transform(df_input['text'].astype(str))\n",
    "    # Get the vocabulary that CountVectorizer actually built (these are our terms if found)\n",
    "    fitted_cv_vocab = cv.get_feature_names_out()\n",
    "    \n",
    "    results = []\n",
    "    # Ensure all terms from original word_list_to_count are in each dict\n",
    "    for i in range(X.shape[0]):\n",
    "        row_counts = {term: 0 for term in word_list_to_count}\n",
    "        doc_vector = X[i]\n",
    "        if doc_vector.nnz > 0: # If any non-zero elements (terms found)\n",
    "            for term_idx, count in zip(doc_vector.indices, doc_vector.data):\n",
    "                term = fitted_cv_vocab[term_idx]\n",
    "                if term in row_counts: # Should always be true if word_list_to_count was basis\n",
    "                    row_counts[term] = count\n",
    "        results.append(row_counts)\n",
    "    return results\n",
    "\n",
    "\n",
    "# M8: Pure Python - Tokenize then Match N-grams\n",
    "def simple_tokenizer(text: str) -> list[str]:\n",
    "    # Basic tokenizer, splits by space and removes empty strings\n",
    "    return [token for token in text.split(' ') if token]\n",
    "\n",
    "def count_words_M8(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    results = []\n",
    "    base_dict = {word: 0 for word in word_list_to_count}\n",
    "    \n",
    "    # Pre-split terms in word_list\n",
    "    split_word_list = {term: term.split(' ') for term in word_list_to_count}\n",
    "\n",
    "    for text in df_input['text']:\n",
    "        text_str = str(text)\n",
    "        current_counts = base_dict.copy()\n",
    "        # For this method, a simple split might be better to match n-grams constructed by space\n",
    "        text_tokens = simple_tokenizer(text_str) \n",
    "        \n",
    "        if not text_tokens: # Handle empty text\n",
    "            results.append(current_counts)\n",
    "            continue\n",
    "\n",
    "        for term, term_tokens in split_word_list.items():\n",
    "            n_term_tokens = len(term_tokens)\n",
    "            if n_term_tokens == 0: continue\n",
    "\n",
    "            count = 0\n",
    "            for i in range(len(text_tokens) - n_term_tokens + 1):\n",
    "                if text_tokens[i:i+n_term_tokens] == term_tokens:\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                current_counts[term] = count\n",
    "        results.append(current_counts)\n",
    "    return results\n",
    "\n",
    "# --- Multiprocessing Helper ---\n",
    "def process_chunk_M2(text_chunk_list: list[str]) -> list[dict[str, int]]:\n",
    "    # This function will be mapped; it needs to be self-contained or use globals carefully\n",
    "    # WORD_LIST and COMPILED_REGEX_M2 are global in the main process\n",
    "    # For multiprocessing, it's better to pass such things or re-initialize if small\n",
    "    # Here, COMPILED_REGEX_M2 is defined from global WORD_LIST.\n",
    "    # Re-define for safety in new processes if needed, or ensure it's inherited.\n",
    "    # Python's multiprocessing on Unix often uses fork, so globals might be available.\n",
    "    # On Windows, it pickles, so globals need to be picklable or passed.\n",
    "    # Let's assume it works or pass WORD_LIST if issues.\n",
    "\n",
    "    current_regex = re.compile(r'\\b(' + '|'.join(re.escape(term) for term in WORD_LIST) + r')\\b')\n",
    "    base_d = {word: 0 for word in WORD_LIST}\n",
    "    chunk_results = []\n",
    "    for text in text_chunk_list:\n",
    "        text_str = str(text)\n",
    "        current_row_counts = base_d.copy()\n",
    "        found_words = current_regex.findall(text_str)\n",
    "        if found_words:\n",
    "            term_counts_in_row = Counter(found_words)\n",
    "            for term, count_val in term_counts_in_row.items():\n",
    "                if term in current_row_counts:\n",
    "                    current_row_counts[term] = count_val\n",
    "        chunk_results.append(current_row_counts)\n",
    "    return chunk_results\n",
    "\n",
    "def count_words_M9_worker(df_chunk): # M2 logic on a df chunk\n",
    "    return count_words_M2(df_chunk, WORD_LIST)\n",
    "\n",
    "\n",
    "def count_words_M9(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    # M2 logic (Python loop + single regex) parallelized\n",
    "    text_list = df_input['text'].tolist()\n",
    "    chunk_size = max(1, len(text_list) // NUM_PROCESSES)\n",
    "    chunks = [text_list[i:i + chunk_size] for i in range(0, len(text_list), chunk_size)]\n",
    "    \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        list_of_results_chunks = pool.map(process_chunk_M2, chunks)\n",
    "    \n",
    "    final_results = [item for sublist in list_of_results_chunks for item in sublist]\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def count_words_M10_worker(df_chunk): # M4 logic on a df chunk\n",
    "    return count_words_M4(df_chunk, WORD_LIST)\n",
    "\n",
    "def count_words_M10(df_input: pd.DataFrame, word_list_to_count: list[str]) -> list[dict[str, int]]:\n",
    "    # M4 logic (Pandas str.findall) parallelized\n",
    "    # Splitting a DataFrame for multiprocessing\n",
    "    df_chunks = np.array_split(df_input, NUM_PROCESSES)\n",
    "    \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
    "        list_of_results_chunks = pool.map(count_words_M10_worker, df_chunks)\n",
    "        \n",
    "    final_results = [item for sublist in list_of_results_chunks for item in sublist]\n",
    "    return final_results\n",
    "\n",
    "# --- Timing and Execution ---\n",
    "methods_to_time = {\n",
    "    \"M0_Loop_StrCount\": count_words_M0,\n",
    "    \"M1_Loop_ReFindall_PerTerm\": count_words_M1,\n",
    "    \"M2_Loop_SingleReFindall\": count_words_M2,\n",
    "    \"M3_PandasApply_SingleRe\": count_words_M3,\n",
    "    \"M4_Pandas_strFindall_SingleRe\": count_words_M4,\n",
    "    \"M5_Pandas_strCount_PerTerm\": count_words_M5,\n",
    "    \"M8_PyLoop_TokenizeMatch\": count_words_M8, # Potentially very slow\n",
    "}\n",
    "if FLASHTEXT_LOADED:\n",
    "    methods_to_time[\"M6_FlashText\"] = count_words_M6\n",
    "if SKLEARN_LOADED:\n",
    "    methods_to_time[\"M7_CountVectorizer\"] = count_words_M7\n",
    "\n",
    "# Multiprocessing methods added separately due to potential for long setup/run times\n",
    "# or if user wants to skip them.\n",
    "# For a fair comparison, the non-parallelized versions are more direct unless specifically testing parallel overhead.\n",
    "# Adding them if explicitly requested or as part of a comprehensive test.\n",
    "# methods_to_time[\"M9_MP_M2\"] = count_words_M9\n",
    "# methods_to_time[\"M10_MP_M4\"] = count_words_M10\n",
    "\n",
    "\n",
    "timings = {}\n",
    "# To ensure correctness, let's get a reference result from one reliable method (M4) on a small subset\n",
    "# And verify other methods against it (structure check done globally)\n",
    "# Check results for first few rows from one method to ensure format\n",
    "# results_m4_sample = count_words_M4(df_main.head(), WORD_LIST)\n",
    "# check_output(results_m4_sample, WORD_LIST)\n",
    "\n",
    "\n",
    "print(f\"\\n--- Starting Benchmark on {NUM_ROWS} rows ---\")\n",
    "print(f\"Using WORD_LIST: {WORD_LIST}\\n\")\n",
    "\n",
    "# Limit number of methods for practical timing in one go, especially slow ones\n",
    "# You can uncomment methods as needed. M0, M1, M8 can be extremely slow.\n",
    "# I'll run a subset that are more likely to be practical.\n",
    "methods_to_run = {\n",
    "    # \"M0_Loop_StrCount\": count_words_M0, # Likely very slow and flawed\n",
    "    \"M1_Loop_ReFindall_PerTerm\": count_words_M1, # Likely very slow\n",
    "    # \"M2_Loop_SingleReFindall\": count_words_M2,\n",
    "    # \"M3_PandasApply_SingleRe\": count_words_M3,\n",
    "    # \"M4_Pandas_strFindall_SingleRe\": count_words_M4, # Expected best\n",
    "    # \"M5_Pandas_strCount_PerTerm\": count_words_M5,\n",
    "}\n",
    "if FLASHTEXT_LOADED:\n",
    "    pass\n",
    "    # methods_to_run[\"M6_FlashText\"] = count_words_M6\n",
    "if SKLEARN_LOADED:\n",
    "    pass\n",
    "    # methods_to_run[\"M7_CountVectorizer\"] = count_words_M7\n",
    "# \"M8_PyLoop_TokenizeMatch\": count_words_M8, # Likely very slow\n",
    "# Add multiprocessing if you want to test their overhead and scaling\n",
    "methods_to_run[\"M9_MP_M2\"] = count_words_M9\n",
    "methods_to_run[\"M10_MP_M4\"] = count_words_M10\n",
    "\n",
    "\n",
    "for name, method_func in methods_to_run.items():\n",
    "    print(f\"Timing {name}...\")\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        # Execute the method\n",
    "        results = method_func(df_main, WORD_LIST)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "        timings[name] = elapsed_time\n",
    "        print(f\"{name} took: {elapsed_time:.4f} seconds.\")\n",
    "        \n",
    "        # Basic validation of output structure (can be commented out for speed after first check)\n",
    "        if not check_output(results, WORD_LIST):\n",
    "             print(f\"!! Output validation failed for {name}\")\n",
    "        # Optional: check if len of results matches df_main\n",
    "        if len(results) != len(df_main):\n",
    "            print(f\"!! Length mismatch for {name}: expected {len(df_main)}, got {len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {name}: {e}\")\n",
    "        timings[name] = \"Error\"\n",
    "\n",
    "print(\"\\n--- Benchmark Results ---\")\n",
    "for name, t in timings.items():\n",
    "    if isinstance(t, str): # Error case\n",
    "        print(f\"{name}: {t}\")\n",
    "    else:\n",
    "        print(f\"{name}: {t:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04ad0973f4eacd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "def create_fast_counter(word_list):\n",
    "    \"\"\"\n",
    "    Create a fast word counter function using precompiled regex.\n",
    "    \n",
    "    Args:\n",
    "        word_list (list): List of words to count\n",
    "    \n",
    "    Returns:\n",
    "        function: Optimized counting function\n",
    "    \"\"\"\n",
    "    # Preprocess words\n",
    "    words_lower = [word.lower() for word in word_list]\n",
    "    \n",
    "    # Create all n-grams (1-grams and 2-grams)\n",
    "    ngrams = words_lower + [' '.join(pair) for pair in combinations(words_lower, 2)]\n",
    "    \n",
    "    # Sort by length (longest first) to avoid partial matches\n",
    "    ngrams.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Precompile regex pattern\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(ngram) for ngram in ngrams) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "    def count_matches(text):\n",
    "        if pd.isna(text) or not text:\n",
    "            return {}\n",
    "        return dict(Counter(match.lower() for match in pattern.findall(str(text))))\n",
    "    \n",
    "    return count_matches\n",
    "\n",
    "# Create optimized counters\n",
    "fast_ai_counter = create_fast_counter(all_terms)\n",
    "fast_social_counter = create_fast_counter(roles)\n",
    "\n",
    "# Apply to dataframe (much faster than swifter for this use case)\n",
    "arxiv_df['ai_word_counts'] = arxiv_df['text'].apply(fast_ai_counter)\n",
    "arxiv_df['social_word_counts'] = arxiv_df['text'].apply(fast_social_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883265b5de41032a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
